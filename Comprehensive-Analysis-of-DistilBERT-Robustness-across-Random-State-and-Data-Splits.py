# -*- coding: utf-8 -*-
"""Untitled19.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ae06_niUkSoEFWgYjSf4ahUFCvYJ943Z
"""

!pip install datasets

"""# 1. Dataset Loading

"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score
from datasets import Dataset, DatasetDict
import torch

# Load the dataset
df = pd.read_csv('Dataset-SA.csv')  # Replace with your actual filename

df.head()

df.info()

df.describe()

"""Data Pre-Processing and EDA"""

df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')
df.dropna(subset=['Review', 'Rate'], inplace=True)

#Create 'Sentiment' column
def map_sentiment(rating):
    if rating >= 4:
        return "positive"
    elif rating <= 2:
        return "negative"
    else:
        return "neutral"

df['Sentiment'] = df['Rate'].apply(map_sentiment)

# Select and rename columns
df = df[['Review', 'Sentiment']]
df.columns = ['text', 'label']

# Print label distribution
print(df['label'].value_counts())

"""# MODEL1 BERT (80-20) rs=42"""

model_name = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)

# Tokenize the text
tokenized_text = tokenizer(
    df['text'].tolist(),
    padding="max_length",
    truncation=True,
    max_length=128,  # Adjust if necessary
    return_tensors="pt"
)

tokenized_data = df.copy()
tokenized_data['input_ids'] = [t.tolist() for t in tokenized_text['input_ids']]
tokenized_data['attention_mask'] = [t.tolist() for t in tokenized_text['attention_mask']]

# Convert labels to numerical format (Important: AFTER tokenization)
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
tokenized_data['label'] = tokenized_data['label'].map(label_mapping)

tokenized_datasets = Dataset.from_pandas(tokenized_data)

# Split the data (using the original pandas DataFrame)
train_valid_df, test_df = train_test_split(tokenized_data, test_size=0.2, random_state=42) #No more stratification here as this is a pandas DataFrame.
train_df, valid_df = train_test_split(train_valid_df, test_size=0.1, random_state=42) #No more stratification here either.

#Convert splits to Datasets. Create DatasetDict.
train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)
test_dataset = Dataset.from_pandas(test_df)

data_dict = DatasetDict({'train':train_dataset, 'validation': valid_dataset, 'test': test_dataset}) #Combine Datasets into a DatasetDict.

# Load the pre-trained model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Define training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=1,              # Adjust as needed
    per_device_train_batch_size=16,  # Adjust as needed
    per_device_eval_batch_size=64,   # Adjust as needed
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="epoch",  # Corrected argument name
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",      # Use weighted f1 for imbalanced data
    report_to="none"  # Disable WandB integration
    # push_to_hub=True,  # Uncomment if using Hugging Face Hub. Requires login.
    # push_to_hub_model_id="your-model-id" # Use your model id if push_to_hub=True. Requires login.
)

# Define the evaluation metric calculation function
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')  # Use weighted F1
    return {"accuracy": accuracy, "f1": f1}

# Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data_dict['train'],  # Access train set from DatasetDict
    eval_dataset=data_dict['validation'],  # Access validation set from DatasetDict
    compute_metrics=compute_metrics  # Pass the metrics function
)

# Train the model
trainer.train()

# Evaluate the model on the test set
predictions = trainer.predict(data_dict['test']) #Make sure to pass a Dataset object to predict.
pred_labels = np.argmax(predictions.predictions, axis=-1)
true_labels = predictions.label_ids
test_f1 = f1_score(true_labels, pred_labels, average='weighted')
test_accuracy = accuracy_score(true_labels, pred_labels)
print(f"Test F1: {test_f1}")
print(f"Test Accuracy: {test_accuracy}")

"""# MODEL2 DistillBERT(80-20)"""

label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
df['label'] = df['label'].map(label_mapping)

model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenized = tokenizer(
    df['text'].tolist(),
    padding="max_length",
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

# Add tokenized input to DataFrame
df['input_ids'] = tokenized['input_ids'].tolist()
df['attention_mask'] = tokenized['attention_mask'].tolist()

train_valid_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, valid_df = train_test_split(train_valid_df, test_size=0.1, random_state=42)

train_dataset = Dataset.from_pandas(train_df)
valid_dataset = Dataset.from_pandas(valid_df)
test_dataset = Dataset.from_pandas(test_df)

data_dict_distilbert = DatasetDict({
    "train": train_dataset,
    "validation": valid_dataset,
    "test": test_dataset
})

model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# ------------------------
# 6. Training arguments
# ------------------------
training_args = TrainingArguments(
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs_distilbert',
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none"
)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    return {"accuracy": accuracy, "f1": f1}

# ------------------------
# 8. Trainer
# ------------------------
trainer_distilbert = Trainer(
    model=model,
    args=training_args,
    train_dataset=data_dict_distilbert['train'],
    eval_dataset=data_dict_distilbert['validation'],
    compute_metrics=compute_metrics
)

trainer_distilbert.train()

predictions_distilbert = trainer_distilbert.predict(data_dict_distilbert['test'])
pred_labels_distilbert = np.argmax(predictions_distilbert.predictions, axis=-1)
true_labels_distilbert = predictions_distilbert.label_ids

# ------------------------
# 10. Final metrics
# ------------------------
test_f1_distilbert = f1_score(true_labels_distilbert, pred_labels_distilbert, average='weighted')
test_accuracy_distilbert = accuracy_score(true_labels_distilbert, pred_labels_distilbert)

print(f"DistilBERT Test Accuracy: {test_accuracy_distilbert:.4f}")
print(f"DistilBERT Test F1 Score: {test_f1_distilbert:.4f}")

conf_matrix_distilbert = confusion_matrix(true_labels_distilbert, pred_labels_distilbert)

sns.heatmap(conf_matrix_distilbert, annot=True, fmt='d', cmap='Purples',
            xticklabels=['negative', 'neutral', 'positive'],
            yticklabels=['negative', 'neutral', 'positive'])

plt.title('Confusion Matrix - DistilBERT')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

"""# MODEL3 DistillBERT(70-30, 60-40)"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming 'true_labels_bert' and 'pred_labels_bert' contain the true and predicted labels for BERT
# Similarly, 'true_labels_distilbert' and 'pred_labels_distilbert' contain the true and predicted labels for DistilBERT

# Confusion matrix for BERT
conf_matrix_bert = confusion_matrix(true_labels, pred_labels)

# Confusion matrix for DistilBERT
conf_matrix_distilbert = confusion_matrix(true_labels_distilbert, pred_labels_distilbert)

# Plot for BERT
plt.figure(figsize=(12, 6))

# BERT Confusion Matrix
plt.subplot(1, 2, 1)
sns.heatmap(conf_matrix_bert, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])
plt.title('Confusion Matrix - BERT')
plt.xlabel('Predicted')
plt.ylabel('True')

# DistilBERT Confusion Matrix
plt.subplot(1, 2, 2)
sns.heatmap(conf_matrix_distilbert, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive'])
plt.title('Confusion Matrix - DistilBERT')
plt.xlabel('Predicted')
plt.ylabel('True')

# Show the plot
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split

# Option C: 70% train, 30% test
train_c_df, test_c_df = train_test_split(df, test_size=0.3, random_state=42)
train_c, val_c = train_test_split(train_c_df, test_size=0.1, random_state=42)

# Option D: 60% train, 40% test
train_d_df, test_d_df = train_test_split(df, test_size=0.4, random_state=42)
train_d, val_d = train_test_split(train_d_df, test_size=0.1, random_state=42)

from datasets import Dataset, DatasetDict

# Convert to DatasetDicts
data_dict_c = DatasetDict({
    'train': Dataset.from_pandas(train_c),
    'validation': Dataset.from_pandas(val_c),
    'test': Dataset.from_pandas(test_c_df)
})

data_dict_d = DatasetDict({
    'train': Dataset.from_pandas(train_d),
    'validation': Dataset.from_pandas(val_d),
    'test': Dataset.from_pandas(test_d_df)
})

from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

model_name = "distilbert-base-uncased"

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    preds = np.argmax(predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds, average='weighted')
    }

def train_distilbert(data_dict, output_dir):
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=64,
        evaluation_strategy="epoch",
        save_strategy="epoch",
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        report_to="none",
        logging_dir=f"{output_dir}/logs",
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=data_dict['train'],
        eval_dataset=data_dict['validation'],
        compute_metrics=compute_metrics
    )

    trainer.train()
    return trainer

def evaluate_distilbert(trainer, dataset):
    predictions = trainer.predict(dataset)
    preds = np.argmax(predictions.predictions, axis=-1)
    labels = predictions.label_ids
    f1 = f1_score(labels, preds, average='weighted')
    acc = accuracy_score(labels, preds)
    return f1, acc

# Option C
trainer_c = train_distilbert(data_dict_c, "./distilbert_c_results")
f1_c, acc_c = evaluate_distilbert(trainer_c, data_dict_c['test'])

# Option D
trainer_d = train_distilbert(data_dict_d, "./distilbert_d_results")
f1_d, acc_d = evaluate_distilbert(trainer_d, data_dict_d['test'])

print(f"Option C - F1: {f1_c:.4f}, Accuracy: {acc_c:.4f}")
print(f"Option D - F1: {f1_d:.4f}, Accuracy: {acc_d:.4f}")

print(f"DistilBERT Test Accuracy: {test_accuracy_distilbert:.6f}")
print(f"DistilBERT Test F1 Score: {test_f1_distilbert:.6f}")
DistilBERT Test Accuracy: 0.985420
DistilBERT Test F1 Score: 0.985266

"""# Comparision for different dataset splits"""

import matplotlib.pyplot as plt
import numpy as np

# Results you provided
splits = ['80/20', '70/30', '60/40']
f1_scores = [0.984287, 0.9848, 0.9849]
accuracies = [0.984478, 0.9850, 0.9850]

x = np.arange(len(splits))  # the label locations
width = 0.35  # the width of the bars

fig, ax = plt.subplots(figsize=(8, 5))
bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='skyblue')
bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='salmon')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('DistilBERT Test Accuracy & F1 Score for Different Train-Test Splits')
ax.set_xticks(x)
ax.set_xticklabels(splits)
ax.set_ylim(0.98, 0.986)
ax.legend()

# Annotate values on top
for bar in bars1 + bars2:
    height = bar.get_height()
    ax.annotate(f'{height:.4f}',
                xy=(bar.get_x() + bar.get_width() / 2, height),
                xytext=(0, 3),  # 3 points vertical offset
                textcoords="offset points",
                ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()

"""MODEL4 DistillBERT for Random State set to false"""

import pandas as pd
import numpy as np
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay
from datasets import Dataset, DatasetDict
import torch
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv("Dataset-SA.csv")

# Preprocess: Clean Rate and Drop NA
df['Rate'] = pd.to_numeric(df['Rate'], errors='coerce')
df.dropna(subset=['Review', 'Rate'], inplace=True)

# Create Sentiment label
def map_sentiment(rating):
    if rating >= 4:
        return "positive"
    elif rating <= 2:
        return "negative"
    else:
        return "neutral"

df['Sentiment'] = df['Rate'].apply(map_sentiment)
df = df[['Review', 'Sentiment']]
df.columns = ['text', 'label']

# Convert labels to numeric
label_mapping = {'negative': 0, 'neutral': 1, 'positive': 2}
df['label'] = df['label'].map(label_mapping)

# Tokenize
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenized = tokenizer(
    df['text'].tolist(),
    padding="max_length",
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

df['input_ids'] = [ids.tolist() for ids in tokenized['input_ids']]
df['attention_mask'] = [mask.tolist() for mask in tokenized['attention_mask']]

# 80/20 split WITHOUT random seed (random_state=None)
train_df, test_df = train_test_split(df, test_size=0.2, random_state=None)
train_df, valid_df = train_test_split(train_df, test_size=0.1, random_state=None)  # 10% of train for validation

# Convert to Hugging Face datasets
train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))
valid_dataset = Dataset.from_pandas(valid_df.reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))

data_dict = DatasetDict({
    "train": train_dataset,
    "validation": valid_dataset,
    "test": test_dataset
})

# Load model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)

# Training arguments
training_args = TrainingArguments(
    output_dir="./results_distilbert_80_20",
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs_distilbert_80_20",
    logging_steps=10,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    report_to="none"
)

# Metrics
def compute_metrics(eval_pred):
    preds, labels = eval_pred
    preds = np.argmax(preds, axis=1)
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average="weighted")
    return {"accuracy": acc, "f1": f1}

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=data_dict["train"],
    eval_dataset=data_dict["validation"],
    compute_metrics=compute_metrics
)

# Train
trainer.train()

# Evaluate on test set
test_preds = trainer.predict(data_dict["test"])
pred_labels = np.argmax(test_preds.predictions, axis=1)
true_labels = test_preds.label_ids

# Final metrics
test_accuracy = accuracy_score(true_labels, pred_labels)
test_f1 = f1_score(true_labels, pred_labels, average="weighted")
print(f"DistilBERT Test Accuracy: {test_accuracy:.6f}")
print(f"DistilBERT Test F1 Score: {test_f1:.6f}")

# Confusion matrix
cm = confusion_matrix(true_labels, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Neutral', 'Positive'])
disp.plot(cmap='Blues', xticks_rotation=45)
plt.title("DistilBERT - 80/20 Split Confusion Matrix")
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Labels for the configurations
labels = ['Random State = 42', 'Random State = False']

# Metrics
accuracy = [0.984478, 0.985420]
f1_score = [0.984287, 0.985266]

x = np.arange(len(labels))  # label locations
width = 0.35  # width of the bars

# Create subplots
fig, ax = plt.subplots(figsize=(8, 5))
rects1 = ax.bar(x - width/2, accuracy, width, label='Accuracy', color='skyblue')
rects2 = ax.bar(x + width/2, f1_score, width, label='F1 Score', color='salmon')

# Add labels, title, and legend
ax.set_ylabel('Score')
ax.set_title('DistilBERT Performance with and without Random State')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.set_ylim([0.983, 0.986])
ax.legend()

# Add text labels above bars
def add_labels(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.6f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 5),
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(rects1)
add_labels(rects2)

plt.tight_layout()
plt.show()