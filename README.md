# ğŸ’¬ BERT-vs-DistilBERT-Sentiment-Analysis  

## ğŸ“Œ Overview  
This project compares **BERT** and **DistilBERT** on a **product reviews dataset** for **sentiment analysis**.  
It demonstrates that **DistilBERT performs nearly identically to BERT** on small datasets while being **faster and more efficient**, followed by a **robustness analysis** across multiple data splits and random states.  

---

## ğŸ§  Objective  
To evaluate how **model size** impacts performance in sentiment classification and determine whether **lighter models like DistilBERT** can match **BERTâ€™s accuracy** on limited data.  

---

## ğŸ“Š Dataset  
- **Type** â†’ Product reviews dataset  
- **Task** â†’ Sentiment classification (Positive / Neutral / Negative)  
- **Size** â†’ Small-scale dataset for lightweight testing  
- **Preprocessing** â†’ Text cleaning, tokenization using Hugging Face Transformers  

---

## âš™ï¸ Approach  
- **Models Used** â†’  
  - `bert-base-uncased`  
  - `distilbert-base-uncased`  
- **Frameworks** â†’ PyTorch, Hugging Face Transformers  
- **Training Details** â†’  
  - Tokenization with padding/truncation  
  - 80/20, 70/30, and 60/40 train-test splits  
  - Evaluation metrics: Accuracy, F1-Score  
- **Robustness Check** â†’ Performance consistency across different random states and splits  

---

## ğŸ† Results  

| Model         | Accuracy | F1 Score | Notes                      |
|----------------|-----------|-----------|-----------------------------|
| **BERT**       | ~98.5%    | ~98.5%    | Baseline model              |
| **DistilBERT** | ~98.4%    | ~98.4%    | Similar performance, faster |

â¡ï¸ **DistilBERT achieved the same performance as BERT on small datasets while being faster and lighter.**  

---

## ğŸ“ˆ Observations  
- Minimal performance gap between BERT and DistilBERT  
- DistilBERT shows **robust performance** across different data splits  
- Ideal choice for **resource-constrained** environments  

---

## ğŸ‘¨â€ğŸ’» Contributors  
- [**Himank Xavier Soren**](https://github.com/himank111)  
